# -*- coding: utf-8 -*-
"""Breast_cancer_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EK-atAI3AZo95tDPWNQ_R7KGY5D2cZKG

# **Breast Cancer Prediction using Feedforward Neural Network**

Data retrieved from: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data
"""

# Import necessary modules/libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Load the csv file using pd.read_csv
breast_cancer = pd.read_csv(r"/content/sample_data/data.csv")

# print head of the dataframe
breast_cancer.head()

# Inspect and pre-analyse the data
# breast_cancer.info() - there are total 33 columns(1 label and the rest is features)
# We are going to train the model based on features
# first we have to check whether there are any null values in the data
print(breast_cancer.iloc[:,:].isnull().sum())

# Next, we're going to remove unnecessary columns that aren't likely to be used for model training
# in this case, column index 0 and 32 need to be removed
breast_cancer = breast_cancer.drop(columns = breast_cancer.iloc[:, [0, 32]], axis=1)
# Check whether updated data
print(breast_cancer.info())

# Next is to change the label from string to numerical
# In this case, M refer to 1(positive) and B refer to 0(negative)
breast_cancer = breast_cancer.replace({'diagnosis': {'M':1, 'B':0}})
# Check whether the string has been change to 0 and 1
print(breast_cancer['diagnosis'].head(30))

# Now we have to split the features and labels
bc_labels = breast_cancer.copy().pop('diagnosis')
bc_features = breast_cancer.copy().drop(['diagnosis'], axis=1)

# Convert features and labels into numpy array
bc_np_labels = np.array(bc_labels)
bc_np_features = np.array(bc_features)

# Now we can perform train_test_split data
SEED=12345
X_train, X_test, y_train, y_test = train_test_split(bc_np_features, bc_np_labels, test_size=0.2,
                                                    random_state=SEED)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Note that we only use .transform() to test split data because I want to rescaled my test data without being biased to the trained model. .transform() function will help
# more explaination can be refer to this link: https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe

# Now we are going to build the model
# Set the nClass variable
nClass = len(np.unique(y_test))

# We are going to use sequential model for feedforward neural network
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(bc_np_features.shape[1])))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(8, activation='relu'))
model.add(tf.keras.layers.Dense(nClass, activation='softmax'))

model.summary()

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

tf.keras.backend.clear_session()

from gc import callbacks
import datetime, os
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

#train the model
BATCH_SIZE= 100
EPOCHS = 100
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=BATCH_SIZE, epochs= EPOCHS, callbacks=[tensorboard_callback])

# Commented out IPython magic to ensure Python compatibility.
# View training accuracy and loss graph via tensorboard
# %tensorboard --logdir logs

# evaluate the model
model.evaluate(X_test, y_test, verbose=2)

tf.keras.backend.clear_session()

# predict the model
np.argmax(model.predict(np.expand_dims(X_test[100], axis=0)))

#plot the graph error 
import matplotlib.pyplot as plt

# plot the graph of training loss vs val_loss
training_loss = history.history['loss']
val_loss = history.history['val_loss']
epoch = history.epoch

plt.plot(epoch, training_loss, label = 'Training Loss')
plt.plot(epoch, val_loss, label = 'Validation Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('error')
plt.show()

#plot the graph of training accuracy vs validation loss
training_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
epoch = history.epoch

plt.plot(epoch, training_accuracy, label = 'Training Accuracy')
plt.plot(epoch, val_accuracy, label = 'Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()

# Saving the trained model
model.save("/content/drive/MyDrive")